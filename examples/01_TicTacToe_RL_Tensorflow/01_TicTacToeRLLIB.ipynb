{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tic Tac Toc + Reinforcement Learning with SageMaker\n",
    "\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this example you'll create a simple game with an agent that plays Tic Tac Toe against a human player. The NPC (Non Player Character) or Bot is a trained **Tensorflow** model. The model will be trained using Reinforcement Learning (OpenAI Gym + RLLib) + **Amazon SageMaker**.\n",
    "\n",
    "After the training process, you'll optimize the model and convert it to a light format using **Amazon SageMaker Neo**. Then with DLR (Runtime) the game will load the model and use its predictions as actions for the Bot.\n",
    "\n",
    "These are the activities for this example:\n",
    "- Prepare an OpenAI Gym custom environment that represents the board and the rules of the game\n",
    "- Prepare an Heuristics (rule based engine) that will play against the agent to make it learn\n",
    "- Train the model using SageMaker\n",
    "- Load the model and play against the Bot using an IPython (widgets) based application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1/4) First we need a new OpenAI Gym that represents the board and the game rules\n",
    "\n",
    "This is a multi-agent experiment, so we need an Env that supports two players simultaneously. \n",
    "To make this work, besides the 9 possible positions in the board we need to create an additional action to represent the player waiting its turn. \n",
    "\n",
    "Run the next cell to visualize the env definition in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize tictactoe/tictactoe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2/4) Heuristics/policy that plays against the agent to make it improve\n",
    "\n",
    "In order to learn how to play and improve its skills, the agent needs to play against a good rival.\n",
    "For that purpose, we can make use of a Policy/Heuristics where we encode some rules and conditions that will simulate a human player.  \n",
    "\n",
    "The policy has a set of rules that are adjusted stochastically over time to behave like a harder or an easier player.\n",
    "\n",
    "Run the next cell to visualize the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize tictactoe/heuristics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3/4) Training the model\n",
    "Now it is time to train our agent using SageMaker Reinforcemenet Learning Estimator.  \n",
    "\n",
    "SageMaker expects that you share a python script with the estimator to execute the training. The following script defines the whole training process using Ray+RLLib + Tensorflow 2 + OpenAI Gym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tictactoe/train.py\n",
    "import sys\n",
    "import subprocess\n",
    "# we need a special package for cleaning our data, lets pip install it first\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", \"sagemaker-training==3.9.2\", \"ray[rllib]==1.2.0\"])\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import argparse\n",
    "import traceback\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "\n",
    "import ray\n",
    "import ray.tune\n",
    "import ray.rllib as rllib\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "\n",
    "import gym\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from gym.envs.registration import register\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "from sagemaker_training import environment, intermediate_output, logging_config, params, files\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "from heuristics import SemiSmartTicTacToeHeuristicsPolicy\n",
    "\n",
    "def freeze_model(saved_model_dir, output_node_names, output_filename):\n",
    "    output_graph_filename = os.path.join(saved_model_dir, output_filename)\n",
    "    initializer_nodes = ''\n",
    "    freeze_graph.freeze_graph(\n",
    "        input_saved_model_dir=saved_model_dir,\n",
    "        output_graph=output_graph_filename,\n",
    "        saved_model_tags = tag_constants.SERVING,\n",
    "        output_node_names=output_node_names,\n",
    "        initializer_nodes=initializer_nodes,\n",
    "        input_graph=None,\n",
    "        input_saver=False,\n",
    "        input_binary=False,\n",
    "        input_checkpoint=None,\n",
    "        restore_op_name=None,\n",
    "        filename_tensor_name=None,\n",
    "        clear_devices=True,\n",
    "        input_meta_graph=False,\n",
    "    )\n",
    "\n",
    "def start_file_sync(env):\n",
    "    \"\"\"Uploads the checkpoints to S3 in background\"\"\"\n",
    "    global logger, intermediate_sync\n",
    "    ## this service will copy all the files, stored in the intermediate dir, to S3\n",
    "    region = os.environ.get(\"AWS_REGION\", os.environ.get(params.REGION_NAME_ENV))\n",
    "    s3_endpoint_url = os.environ.get(params.S3_ENDPOINT_URL, None)\n",
    "\n",
    "    logger.info(\"Starting intermediate sync. %s: %s - %s\" % (region, env.sagemaker_s3_output(), s3_endpoint_url))\n",
    "    intermediate_sync = intermediate_output.start_sync(\n",
    "        env.sagemaker_s3_output(), region, endpoint_url=s3_endpoint_url\n",
    "    )\n",
    "    \n",
    "def get_latest_checkpoint(env, algo):\n",
    "    \"\"\"Scan the intermediate dir and get the latest checkpoint\"\"\"\n",
    "    global logger\n",
    "    logger.info(\"Latest checkpoint\")\n",
    "    # get the latest experiment\n",
    "    experiments = glob.glob(os.path.join(env.output_intermediate_dir,'training', f'{algo}*'))\n",
    "    experiments.sort(key=lambda x: [int(c) if c.isdigit() else c for c in ''.join(x.replace('-','').split('_')[-2:])])\n",
    "\n",
    "    if len(experiments) > 0:\n",
    "        exp_name = experiments[-1]\n",
    "\n",
    "        chkpts = [c for c in glob.glob(f'{exp_name}/checkpoint*')]\n",
    "        chkpts.sort(key=lambda x: [int(c) if c.isdigit() else c for c in re.split('(\\d+)', x)])\n",
    "\n",
    "        if len(chkpts) == 0: raise Exception(\"No checkpoint found!\")\n",
    "        ckpt_path=chkpts[-1]\n",
    "        ckpt_meta_filename=ckpt_path.split('/')[-1].split('_')\n",
    "        ckpt_meta_filename=f'{ckpt_meta_filename[0]}-{int(ckpt_meta_filename[1])}'\n",
    "        logger.info(f'{ckpt_path}/{ckpt_meta_filename}')\n",
    "        return ckpt_path, ckpt_meta_filename\n",
    "\n",
    "def save_model(env_vars, experiment_params):\n",
    "    \"\"\"Load a checkpoint and export it as a TF1.15 model SavedModel\"\"\"\n",
    "    global logger\n",
    "    config = copy.deepcopy(experiment_params)['training']['config']\n",
    "\n",
    "    config[\"monitor\"] = False\n",
    "    config[\"num_workers\"] = 1\n",
    "    config[\"num_gpus\"] = 0\n",
    "    logger.info(experiment_params)\n",
    "    algo = experiment_params['training']['run']\n",
    "    env_name = experiment_params['training']['env']\n",
    "    logger.info(f'{algo} - {env_name}')\n",
    "    cls = get_agent_class(algo)        \n",
    "    agent = cls(env=env_name, config=config)\n",
    "    \n",
    "    ckpt_path, ckpt_meta_filename = get_latest_checkpoint(env_vars, algo)\n",
    "    \n",
    "    logger.info('Restoring agent...')\n",
    "    agent.restore(os.path.join(ckpt_path, ckpt_meta_filename))\n",
    "    logger.info(f'Exporting model to {env_vars.model_dir}...')\n",
    "    \n",
    "    agent.export_policy_model(os.path.join(env_vars.output_intermediate_dir,'saved_model'), 'agent_x')\n",
    "    freeze_model(os.path.join(env_vars.output_intermediate_dir,'saved_model'), 'agent_x/fc_out/BiasAdd', 'frozen.pb')\n",
    "    copyfile(os.path.join(env_vars.output_intermediate_dir,'saved_model', 'frozen.pb'), os.path.join(env_vars.model_dir, 'frozen.pb'))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    env_vars = environment.Environment()\n",
    "    parser = argparse.ArgumentParser()\n",
    "    logging_config.configure_logger(env_vars.log_level)\n",
    "    \n",
    "    parser.add_argument(\"--log-level\", type=int, default=0)\n",
    "    parser.add_argument(\"--record-videos\", type=bool, default=False)\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=max(env_vars.num_cpus-1, 3))\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=env_vars.num_gpus)\n",
    "    parser.add_argument(\"--batch-mode\", type=str, default=\"complete_episodes\")\n",
    "    parser.add_argument(\"--episode-reward-mean\", type=float, default=3.5)\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=0.001)\n",
    "    parser.add_argument(\"--init-seed\", type=int, default=-1)\n",
    "    parser.add_argument(\"--refining-iter\", type=int, default=4)\n",
    "    args,unknown = parser.parse_known_args()\n",
    "\n",
    "    seed=args.init_seed if args.init_seed != -1 else None\n",
    "    \n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    logger = logging_config.get_logger()\n",
    "    intermediate_sync = None\n",
    "\n",
    "    env_name='TicTacToeEnv-v0'\n",
    "    register(\n",
    "        id=env_name,\n",
    "        entry_point='tictactoe:TicTacToeEnv'\n",
    "    )\n",
    "    env = gym.make(env_name)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    experiment_params = {\n",
    "        \"training\": {\n",
    "            \"env\": env_name,\n",
    "            \"run\": \"A3C\",\n",
    "            \"stop\": {\n",
    "                \"episode_reward_mean\": args.episode_reward_mean,\n",
    "            },\n",
    "            \"local_dir\": env_vars.output_intermediate_dir,\n",
    "            \"checkpoint_at_end\": True,\n",
    "            \"checkpoint_freq\": 60,\n",
    "            #\"export_formats\": [\"h5\"],\n",
    "            \"config\": {            \n",
    "                \"log_level\": args.log_level,\n",
    "                \"monitor\": args.record_videos,\n",
    "                #\"framework\": \"tfe\",\n",
    "                \"lr\": args.learning_rate,\n",
    "                \"model\": {\n",
    "                    # https://docs.ray.io/en/master/rllib-models.html#default-model-config-settings\n",
    "                },\n",
    "                \"multiagent\": {\n",
    "                    \"policies\": {\n",
    "                        \"agent_x\": (None, env.observation_space, env.action_space, {}),\n",
    "                        \"agent_o\": (SemiSmartTicTacToeHeuristicsPolicy, env.observation_space, env.action_space, {})\n",
    "                    },\n",
    "                    \"policy_mapping_fn\": lambda x: x,\n",
    "                    \"policies_to_train\": [\"agent_x\"],                \n",
    "                },            \n",
    "                \"num_workers\": args.num_workers,\n",
    "                \"num_gpus\": args.num_gpus,\n",
    "                \"batch_mode\": args.batch_mode,\n",
    "                \"seed\": seed\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        start_file_sync(env_vars)\n",
    "        # main program\n",
    "        ray.init()\n",
    "        ray.tune.register_env(env_name, lambda x: env)\n",
    "        ray.tune.run_experiments(copy.deepcopy(experiment_params))\n",
    "        for i in range(args.refining_iter):\n",
    "            seed = int(time.time())\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)    \n",
    "            env.seed(seed)\n",
    "            algo = experiment_params['training']['run']\n",
    "            ckpt_path, ckpt_meta_filename = get_latest_checkpoint(env_vars, algo)\n",
    "            experiment_params['training']['config']['seed'] = seed\n",
    "            experiment_params['training']['restore'] = os.path.join(ckpt_path, ckpt_meta_filename)\n",
    "            ray.tune.run_experiments(copy.deepcopy(experiment_params))\n",
    "        save_model(env_vars, experiment_params)\n",
    "        ray.shutdown()\n",
    "        \n",
    "        files.write_success_file()\n",
    "        logger.info(\"Reporting training SUCCESS\")\n",
    "    except Exception as e:\n",
    "        failure_msg = \"framework error: \\n%s\\n%s\" % (traceback.format_exc(), str(e))\n",
    "        logger.error(\"Reporting training FAILURE: %s\" % failure_msg)\n",
    "        files.write_failure_file(failure_msg)\n",
    "    finally:\n",
    "        if intermediate_sync:\n",
    "            intermediate_sync.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the agent using SageMaker RL container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "# S3 bucket\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_session.default_bucket()  \n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "\n",
    "# create a descriptive job name \n",
    "aws_region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "import time\n",
    "\n",
    "image_name=f\"462105765813.dkr.ecr.{aws_region}.amazonaws.com/sagemaker-rl-ray-container:ray-1.1.0-tf-gpu-py36\"\n",
    "estimator = RLEstimator(\n",
    "    image_uri=image_name,\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir='tictactoe',\n",
    "    role=role,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    max_run=60*(60 * 2),\n",
    "    instance_count=1,\n",
    "    output_path=s3_output_path,\n",
    "    metric_definitions=RLEstimator.default_metric_definitions(RLToolkit.RAY),\n",
    "    hyperparameters={\n",
    "        \"log-level\": 20,\n",
    "        \"record-videos\": False,\n",
    "        \"batch-mode\": \"complete_episodes\",\n",
    "        \"episode-reward-mean\": 5.0,\n",
    "        \"learning-rate\": 0.0001,\n",
    "        \"init-seed\": 1, # seed == 1 makes the agent learn faster but it gets biased\n",
    "        \"refining-iter\": 5 # refining iterations are to make the agent generalize to random matches\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kick-off the training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(wait=True)\n",
    "job_name = estimator.latest_training_job.job_name\n",
    "print(\"Training job: %s\" % job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4/4) Playing against the agent\n",
    "\n",
    "There is a simple IPython application [Game](tictactoe/game.py) that we'll use to load the model, render the board an let us play against the agent.\n",
    "\n",
    "This application expects a model optimized with SageMaker NEO.\n",
    "So, let's compile the Tensorflow model with SageMaker Neo and use it in our local Application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "# Change the target OS + Arch depending on the device you'll run this application\n",
    "os='LINUX'\n",
    "arch='X86_64' # ARM64\n",
    "model_name='tic-tac-toe'\n",
    "filename=f'model-{os}_{arch}.tar.gz'\n",
    "\n",
    "s3_uri=f'{estimator.output_path}{estimator.latest_training_job.name}/output/model.tar.gz'\n",
    "s3_uri_out=f's3://{sagemaker_session.default_bucket()}/{model_name}-tensorflow/optimized/{filename}'\n",
    "\n",
    "compilation_job_name = f'{model_name}-tensorflow-{int(time.time()*1000)}'\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_uri,\n",
    "        'DataInputConfig': '{\"agent_x/observations\": [1,10]}',\n",
    "        'Framework': 'TENSORFLOW'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': s3_uri_out,\n",
    "        'TargetPlatform': { \n",
    "            'Os': os, \n",
    "            'Arch': arch,\n",
    "            #'Accelerator': 'NVIDIA'  # comment this if you don't have an Nvidia GPU\n",
    "        },\n",
    "        # Comment or change the following line depending on your edge device\n",
    "        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n",
    "        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}' # Jetpack 4.4.1\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and unpack the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $s3_uri_out /tmp/\n",
    "!rm -rf model_neo && mkdir model_neo\n",
    "!tar -xzvf /tmp/$filename -C model_neo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install DLR (runtime that loads the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install DLR, the runtime required to load the model\n",
    "!pip install -U dlr==1.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have fun!! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tictactoe.game import TicTacToeGame\n",
    "game = TicTacToeGame()\n",
    "game.run()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
