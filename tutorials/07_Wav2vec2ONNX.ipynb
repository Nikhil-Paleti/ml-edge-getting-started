{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b00df02-b380-440f-9ca4-1f68c1153321",
   "metadata": {},
   "source": [
    "# Wav2Vec2 (Speech Recognition) optimized to ONNX\n",
    "**SageMaker Studio Kernel**: Data Science\n",
    "\n",
    "In this tutorial you learn how to convert a TF/Hugging Face transformer for Speech Recognition to ONNX. ONNX is an important optimization tool that allows you to run ML models everywhere (cloud, on-prem or at the edge) with good performance. These are the steps of this tutorial:\n",
    "   - Install some required libraries\n",
    "   - Compile the model with ONNX and then quantize the model\n",
    "   - Run a benchmark to see how the performance was improved"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d52c8d-90aa-42cb-be20-1b25f486aee8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1) Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb38fd7-4636-4855-87e5-70e4363d378f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt-get update -y && apt install -y build-essential python-soundfile\n",
    "%pip install tensorflow==2.5 onnx==1.7 onnxruntime==1.9.0 soundfile\n",
    "%pip install -q git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1a61c-5deb-42dc-84fb-50308bbc3726",
   "metadata": {},
   "source": [
    "## 2) Run a SageMaker Processing Job to convert the TF2.5 model to ONNX 1.7\n",
    "Convert Tensorflow model to ONNX format via SageMaker TensorFlowProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f57510-5c8d-4203-831e-dd9eca673403",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171b55a-a8ef-474f-899d-60f41604064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/processing-script.py\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import os\n",
    "import shutil\n",
    "import tf2onnx\n",
    "\n",
    "from wav2vec2 import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
    "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"vasudevgupta/finetuned-wav2vec2-960h\")\n",
    "\n",
    "AUDIO_MAXLEN = 50000\n",
    "ONNX_PATH = \"wav2vec2.onnx\"\n",
    "\n",
    "# ONNX graph is described following the specifications of the opset number\n",
    "\n",
    "input_signature = (tf.TensorSpec((None, AUDIO_MAXLEN), tf.float32, name=\"speech\"),)\n",
    "model = tf2onnx.convert.from_keras(model, input_signature=input_signature, output_path=ONNX_PATH)\n",
    "\n",
    "onnx.save(model[0],'/opt/ml/processing/output/wav2vec2.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adafa702-6920-4016-90b7-0a0ef72866c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "tensorflow==2.5\n",
    "tf2onnx==1.11.1\n",
    "onnx==1.7\n",
    "numpy==1.19.2\n",
    "git+https://github.com/vasudevgupta7/gsoc-wav2vec2@main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edaa867-a2bd-4c81-8a34-cb93e4a41f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlowProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "S3_EXP_PATH = 'TF2ONNX'\n",
    "\n",
    "tp = TensorFlowProcessor(\n",
    "    framework_version='2.5',\n",
    "    role=get_execution_role(),\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name='ml-edge-tf25',\n",
    "    image_uri=f'763104351884.dkr.ecr.{region}.amazonaws.com/tensorflow-training:2.5.0-cpu-py37-ubuntu18.04-v1.0',\n",
    "    py_version='py37',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c2f601-ac84-412d-8261-0fb932de4b4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run the processing job\n",
    "tp.run(\n",
    "    code='processing-script.py',\n",
    "    source_dir='code',\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='wav2vec.onnx',\n",
    "            source='/opt/ml/processing/output',\n",
    "            destination=f's3://{BUCKET}/{S3_EXP_PATH}/output'\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755aac4c-483f-4c36-a12f-2b100471c02d",
   "metadata": {},
   "source": [
    "### Download the ONNX model file. \n",
    "Also, let's download an audio sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ff9274-1e24-4284-974d-d5633390143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "\n",
    "sagemaker_session.download_data('model', bucket = BUCKET,key_prefix=f'{S3_EXP_PATH}/output')\n",
    "\n",
    "if not os.path.isfile('sample.wav'):\n",
    "    urllib.request.urlretrieve('https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/sample.wav', 'sample.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072cb0df-9ced-4ef6-8489-9cd6c0f32c8c",
   "metadata": {},
   "source": [
    "### Load the audio sample and build a tensor (padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "AUDIO_MAXLEN = 50000\n",
    "FILENAME = \"sample.wav\"\n",
    "\n",
    "speech, _ = sf.read(FILENAME)\n",
    "speech = tf.constant(speech, dtype=tf.float32)\n",
    "speech = processor(speech)[None]\n",
    "\n",
    "padding = tf.zeros((speech.shape[0], AUDIO_MAXLEN - speech.shape[1]))\n",
    "speech = tf.concat([speech, padding], axis=-1)\n",
    "speech.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f25a6-8e90-45d5-a60b-c2a5208cab08",
   "metadata": {},
   "source": [
    "## 3) Benchmark on CPU - TF vs ONNX vs ONNX quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad64f9b-7f52-41ff-b437-187f456f7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def timeit(prefix=\"Time taken:\", iterations=1):\n",
    "    start = time.time()\n",
    "    yield\n",
    "    time_taken = (time.time() - start)/iterations\n",
    "    print(prefix, time_taken, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7210f4a7-20b1-463f-845b-8f4022fcd0e7",
   "metadata": {},
   "source": [
    "### Quantize ONNX model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ef82c-1924-40fd-adee-438ad5cdc771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic\n",
    "\n",
    "# https://github.com/microsoft/onnxruntime/issues/3130\n",
    "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
    "    onnx_opt_model = onnx.load(onnx_model_path)\n",
    "    quantize_dynamic(\n",
    "        onnx_model_path,\n",
    "        quantized_model_path,\n",
    "         #nodes_to_exclude=['ConvInteger*','MatMulInteger*'],\n",
    "         #nodes_to_quantize=[],\n",
    "         weight_type=QuantType.QUInt8,\n",
    "         #extra_options={\"WeightSymmetric\": False, \"MatMulConstBOnly\": True}\n",
    "    )   \n",
    "    print(f\"quantized model saved to:{quantized_model_path}\")\n",
    "quantize_onnx_model(\"./model/wav2vec2.onnx\", \"./model/wav2vec2_quant.onnx\")\n",
    "!du -sh model/*.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0e6a3-8bf6-481c-92ed-9da0909e07c5",
   "metadata": {},
   "source": [
    "### Load the original TF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc75c4-1edc-49d0-afba-489652ad3bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from wav2vec2 import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
    "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
    "model_tf = Wav2Vec2ForCTC.from_pretrained(\"vasudevgupta/finetuned-wav2vec2-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2745ae41-06d1-4391-a832-4f614d7a1396",
   "metadata": {},
   "source": [
    "### Load ONNX & ONNX quantized models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f2cf26-2f63-4f2f-9af6-27a6f87e4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as rt\n",
    "\n",
    "model_quant_onnx= \"./model/wav2vec2_quant.onnx\"\n",
    "model_onnx= \"./model/wav2vec2.onnx\"\n",
    "\n",
    "session_qt = rt.InferenceSession(model_quant_onnx)\n",
    "session_x = rt.InferenceSession(model_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739ad531-c1fe-4e36-ac66-95dc14fe4e82",
   "metadata": {},
   "source": [
    "### Warmup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9415cc0b-436a-4b3c-a047-b5195ea2bc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1,y2,y3 = model_tf(speech), session_x.run(None, {\"speech\": speech.numpy()})[0], session_qt.run(None, {\"speech\": speech.numpy()})[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bbb99-7d9d-4563-a1cf-6ee52eba0fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Benchmark all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d395ed0-6116-402e-819e-f93d11741743",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=10\n",
    "with timeit(prefix=\"TF 2.5 model - time take:\", iterations=iterations):\n",
    "    [model_tf(speech) for i in range(iterations)]\n",
    "    \n",
    "with timeit(prefix=\"ONNX time taken:\", iterations=iterations):\n",
    "    [session_x.run(None, {\"speech\": speech.numpy()})[0] for i in range(iterations)]\n",
    "\n",
    "with timeit(prefix=\"ONNX quantized time taken:\", iterations=iterations):\n",
    "    [session_qt.run(None, {\"speech\": speech.numpy()})[0] for i in range(iterations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da548f-d1bb-449f-b335-cc80886a3b83",
   "metadata": {},
   "source": [
    "### Check output tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf_outputs = model_tf(speech)\n",
    "onnx_outputs = session_x.run(None, {\"speech\": speech.numpy()})[0]\n",
    "onnx_quant_outputs = session_qt.run(None, {\"speech\": speech.numpy()})[0]\n",
    "\n",
    "assert np.allclose(tf_outputs, onnx_outputs, atol=1e-2)\n",
    "assert np.allclose(tf_outputs, onnx_quant_outputs, atol=1e-2) # error ~= 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e6df9-fcbe-48c5-807e-95b84bf68f18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fec62-3e82-4513-8ac3-c190095fe1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import Audio\n",
    "tokenizer = Wav2Vec2Processor(is_tokenizer=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f4c692-b053-4c8a-a1fa-f93fe04a3551",
   "metadata": {},
   "source": [
    "#### TF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0386c0d-2bae-4ca2-a8ce-6215d38f6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(tf_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "print(\"prediction from TF:\", prediction)\n",
    "Audio(filename=FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08d7b5-9fde-4978-aa6b-ce9363815627",
   "metadata": {},
   "source": [
    "##### ONNX Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d87ad-0ce6-4671-b7bb-b06834ef3d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(onnx_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "print(\"prediction from ONNX:\", prediction)\n",
    "Audio(filename=FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee64c16-359b-4047-bef9-705babbe176b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### ONNX Quantized Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e656d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.argmax(onnx_quant_outputs, axis=-1)\n",
    "prediction = tokenizer.decode(prediction.squeeze().tolist())\n",
    "print(\"prediction from Quantized ONNX:\", prediction)\n",
    "Audio(filename=FILENAME)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
