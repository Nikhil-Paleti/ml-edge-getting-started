{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surface defect detection with semantic segmentation on Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will train a TensorFlow2 model with unet[https://arxiv.org/abs/1505.04597] architecture for surface defect detection images. We will then compile and prepare the model for deployment with SageMaker Edge. \n",
    "For a complete workshop, check out the repo[https://github.com/aws-samples/amazon-sagemaker-edge-defect-detection-computer-vision] \n",
    "1. Prerequisites : Prepare the dataset (see data_preparation notebook)\n",
    "2. Train the model with SM \n",
    "3. Compile model with SM Neo\n",
    "4. Prepare deployment package for SM Edge Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U tensorflow==2.2.0 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U botocore boto3 awscli --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import Session\n",
    "import boto3 \n",
    "\n",
    "session = Session()\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = 'sm-edge-getting-started-unet-%s' % (account_id)\n",
    "prefix = 'segmentation'\n",
    "region = session.boto_region_name\n",
    "# Define IAM role\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "import os\n",
    "\n",
    "role = get_execution_role()\n",
    "from sagemaker.s3 import S3Uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "EPOCHS = 100\n",
    "BATCH = 8\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "def load_data(path, split=0.1):\n",
    "    print(os.path.join(path, \"images/*\"))\n",
    "    images = sorted(glob(os.path.join(path, \"images/*\")))\n",
    "    masks = sorted(glob(os.path.join(path, \"masks/*\")))\n",
    "\n",
    "    total_size = len(images)\n",
    "    valid_size = int(split * total_size)\n",
    "    test_size = int(split * total_size)\n",
    "    print(total_size)\n",
    "    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n",
    "    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n",
    "\n",
    "    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n",
    "    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/kolektor-preprocessed/semantic-segmentation/'\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in train_x:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/train/images'.format(bucket, prefix))\n",
    "for file in train_y:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/train/masks'.format(bucket, prefix))\n",
    "for file in valid_x:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/val/images'.format(bucket, prefix))\n",
    "for file in valid_y:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/val/masks'.format(bucket, prefix))\n",
    "for file in test_x:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/test/images'.format(bucket, prefix))\n",
    "for file in test_y:\n",
    "    S3Uploader.upload(file, 's3://{}/{}/data/test/masks'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "train_input = TrainingInput('s3://{0}/{1}/data/train/'.format(\n",
    "    bucket, prefix), content_type='image/png')\n",
    "val_input = TrainingInput('s3://{0}/{1}/data/val/'.format(\n",
    "    bucket, prefix), content_type='image/png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TensorFlow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source_dir/train_tf.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "model_dir = '/opt/ml/model'\n",
    "hyperparameters = {'epochs': 10, 'batch_size': 8, 'learning_rate': 0.0001}\n",
    "estimator = TensorFlow(source_dir='source_dir',\n",
    "                             entry_point='train_tf.py',\n",
    "                             model_dir=model_dir,\n",
    "                             instance_type='ml.c5.xlarge',\n",
    "                             #instance_type='local',\n",
    "                             instance_count=1,\n",
    "                             hyperparameters=hyperparameters,\n",
    "                             role=role,\n",
    "                             output_path='s3://{}/{}/{}'.format(bucket, prefix, 'tf_model'),\n",
    "                             framework_version='2.2.0',\n",
    "                             py_version='py37',\n",
    "                             script_mode=True)\n",
    "\n",
    "inputs = {'train':train_input, 'validation':val_input}\n",
    "pred=estimator.fit(inputs,job_name='unet-segmentation-tf2-5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = 's3://{}/{}/{}'.format(bucket, prefix, 'tf_model') + '/unet-segmentation-tf2-5/output/model.tar.gz'\n",
    "print('model uploaded to: {}'.format(model_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model with SageMaker Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import time\n",
    "sm_client = boto3.client('sagemaker')\n",
    "s3_path = model_data\n",
    "compilation_job_name = '%s-%d' % ('unet', int(time.time()*1000))\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_path,\n",
    "        'DataInputConfig': '{\"input_image\":[1,%d,%d,%d]}' % (3,256, 256),\n",
    "        'Framework': 'KERAS'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': 's3://%s/%s/optimized/' % (bucket, prefix),\n",
    "        'TargetPlatform': { 'Os': 'LINUX', 'Arch': 'X86_64' }\n",
    "        #'TargetPlatform': { 'Os': 'LINUX', 'Arch': 'ARM64', 'Accelerator': 'NVIDIA' },\n",
    "        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}'\n",
    "        #'TargetPlatform': { 'Os': 'LINUX', 'Arch': 'ARM64'},\n",
    "        #'TargetDevice': 'ml_c5'\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)\n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package the model with SageMaker Edge Manager "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_packaging_job_name='%s-%d' % ('unet', int(time.time()*1000))\n",
    "model_version='1.0'\n",
    "model_name='unet'\n",
    "num_classes=1\n",
    "resp = sm_client.create_edge_packaging_job(\n",
    "    EdgePackagingJobName=edge_packaging_job_name,\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    ModelName=model_name,\n",
    "    ModelVersion=model_version,\n",
    "    RoleArn=role,\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': 's3://%s/%s-%dx%d-%d/' % (bucket, prefix, 256, 256, num_classes)\n",
    "    }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)\n",
    "    if resp['EdgePackagingJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['EdgePackagingJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('s3://%s/%s-%dx%d-%d/' % (bucket_name, prefix, 256, 256, num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
